import os
import time  # ã€æ–°å¢ã€‘å¼•å…¥timeæ¨¡å—ç”¨äºé€€å‡ºå»¶è¿Ÿ

# å¿…é¡»å¤„äºæ–‡ä»¶æœ€é¡¶ç«¯ï¼šç¯å¢ƒé…ç½®
os.environ["DIFFUSERS_USE_PEFT_BACKEND"] = "1"
os.environ["HF_HUB_DISABLE_SYMLINKS_WARNING"] = "1"
os.environ["HF_HUB_OFFLINE"] = "1"

import sys
import torch
import psutil
import random
import re
import uuid
import gc
from datetime import datetime
from PIL import Image, ImageFilter, ImageOps, ImageEnhance

# é…ç½®åŸºç¡€è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.append(current_dir)

# ç›®å½•é…ç½®
DEFAULT_MODEL_PATH = os.path.join(current_dir, "ckpts", "Z-Image-Turbo")
LORA_ROOT = os.path.join(current_dir, "lora")
OUTPUT_ROOT = os.path.join(current_dir, "outputs")
MOD_VAE_DIR = os.path.join(current_dir, "Mod", "vae")
MOD_TRANS_DIR = os.path.join(current_dir, "Mod", "transformer")
for p in [LORA_ROOT, OUTPUT_ROOT, MOD_VAE_DIR, MOD_TRANS_DIR]:
    os.makedirs(p, exist_ok=True)

try:
    import gradio as gr
    from diffusers import ZImagePipeline, ZImageImg2ImgPipeline, AutoencoderKL
    from safetensors.torch import load_file
except ImportError as e:
    print(f"âŒ æ ¸å¿ƒåº“å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# ==========================================
# è®¾å¤‡æ¢æµ‹ä¸ç¡¬ä»¶æŠ¥å‘Š
# ==========================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.bfloat16 if DEVICE == "cuda" else torch.float32
is_interrupted = False

print("\n" + "="*50)
if DEVICE == "cuda":
    GPU_NAME = torch.cuda.get_device_name(0)
    TOTAL_VRAM = torch.cuda.get_device_properties(0).total_memory
    print(f"âœ… è¿è¡Œæ¨¡å¼: [ GPU ]")
    print(f"æ ¸å¿ƒå‹å·: {GPU_NAME}")
    print(f"æ˜¾å­˜æ€»é‡: {TOTAL_VRAM/1024**3:.2f} GB")
else:
    TOTAL_VRAM = 0
    print(f"âš ï¸ è¿è¡Œæ¨¡å¼: [ CPU ]")
print("="*50 + "\n")

# ==========================================
# æ˜¾å­˜ä¸å·¥å…·å‡½æ•°
# ==========================================
def get_vram_info():
    if DEVICE == "cuda":
        reserved = torch.cuda.memory_reserved(0)
        allocated = torch.cuda.memory_allocated(0)
        usage_pct = (reserved / TOTAL_VRAM) * 100 if TOTAL_VRAM > 0 else 0
        vram_str = (
            f"æ˜¾å­˜å ç”¨: {usage_pct:.1f}% "
            f"({reserved/1024**3:.2f}GB / {TOTAL_VRAM/1024**3:.2f}GB)"
        )
    else:
        usage_pct = 0
        vram_str = "æ˜¾å­˜å ç”¨: CPU æ¨¡å¼"

    mem = psutil.virtual_memory()
    ram_str = (
        f"å†…å­˜å ç”¨: {mem.percent:.1f}% "
        f"({(mem.total - mem.available)/1024**3:.2f}GB / {mem.total/1024**3:.2f}GB)"
    )
    status = f"{vram_str} ï½œ {ram_str}"
    return usage_pct, status


def auto_flush_vram(threshold=90):
    usage_pct, _ = get_vram_info()
    if usage_pct > threshold:
        gc.collect()
        torch.cuda.empty_cache()
        return True
    return False

def scan_lora_files():
    if not os.path.exists(LORA_ROOT): return []
    return sorted([f for f in os.listdir(LORA_ROOT) if f.lower().endswith(".safetensors")])

def scan_model_items(base_path):
    if not os.path.exists(base_path): return []
    items = []
    for f in os.listdir(base_path):
        full_path = os.path.join(base_path, f)
        if os.path.isdir(full_path):
            items.append(f)
        elif f.lower().endswith((".safetensors", ".bin", ".pt")):
            items.append(f)
    return sorted(items)

# ==========================================
# å…¨å±€ LoRA æ–‡ä»¶åˆ—è¡¨ (å¯åŠ¨æ—¶æ‰«æ)
# ==========================================
LORA_FILES = scan_lora_files()
print(f"ğŸ” å·²æ£€æµ‹åˆ° {len(LORA_FILES)} ä¸ª LoRA æ–‡ä»¶ï¼Œæ­£åœ¨ç”Ÿæˆç‹¬ç«‹æ§ä»¶...")
if len(LORA_FILES) > 30:
    print("âš ï¸ è­¦å‘Š: LoRA æ•°é‡è¾ƒå¤šï¼Œç”Ÿæˆç•Œé¢å¯èƒ½éœ€è¦å‡ ç§’é’Ÿ...")

# ==========================================
# æ¨¡å‹ç®¡ç†å™¨ (ä¿®æ”¹ç‰ˆï¼šæ”¯æŒç‹¬ç«‹æƒé‡)
# ==========================================
class ModelManager:
    def __init__(self):
        self.pipe = None 
        self.current_state = {
            "mode": None,      
            "t_choice": None,  
            "v_choice": None,  
        }
        self.current_loras = []
        self.current_weights_map = {} 

    def _clear_pipeline(self):
        if self.pipe is not None:
            print(f"ğŸ§¹ æ­£åœ¨é”€æ¯æ—§ç®¡é“ä»¥é‡Šæ”¾æ˜¾å­˜...")
            try:
                self.pipe.unload_lora_weights()
            except:
                pass
            del self.pipe
            self.pipe = None
        if hasattr(sys, 'last_traceback'):
            del sys.last_traceback
        for _ in range(3):
            gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        if DEVICE == "cuda":
            res = torch.cuda.memory_reserved(0) / 1024**3
            print(f"âœ¨ æ˜¾å­˜å·²æ·±åº¦æ¸…ç†ï¼Œå½“å‰å ç”¨: {res:.2f} GB")

    def _init_pipeline_base(self, mode):
        if mode == 'txt':
            print("ğŸš€ åˆå§‹åŒ–åŸºç¡€ Pipeline (æ–‡ç”Ÿå›¾)...")
            return ZImagePipeline.from_pretrained(DEFAULT_MODEL_PATH, torch_dtype=DTYPE, local_files_only=True)
        else:
            print("ğŸš€ åˆå§‹åŒ–åŸºç¡€ Pipeline (å›¾ç”Ÿå›¾)...")
            return ZImageImg2ImgPipeline.from_pretrained(DEFAULT_MODEL_PATH, torch_dtype=DTYPE, local_files_only=True)

    def _inject_components(self, pipe, t_choice, v_choice):
        if t_choice != "default":
            t_path = os.path.join(MOD_TRANS_DIR, t_choice)
            if os.path.isfile(t_path):
                print(f"ğŸ“¦ è½½å…¥ Transformer: {t_choice}")
                state_dict = load_file(t_path, device="cpu")
                processed = {}
                prefix = "model.diffusion_model."
                for k, v in state_dict.items():
                    new_k = k[len(prefix):] if k.startswith(prefix) else k
                    processed[new_k] = v.to(DTYPE)
                pipe.transformer.load_state_dict(processed, strict=False, assign=True)
                del state_dict, processed, v
                gc.collect()

        if v_choice != "default":
            vae_path = os.path.join(MOD_VAE_DIR, v_choice)
            print(f"ğŸ“¦ è½½å…¥ VAE: {v_choice}")
            if os.path.isfile(vae_path):
                pipe.vae = AutoencoderKL.from_single_file(vae_path, torch_dtype=DTYPE)
            else:
                pipe.vae = AutoencoderKL.from_pretrained(vae_path, torch_dtype=DTYPE)
        return pipe

    def _apply_loras(self, pipe, selected_loras, weights_map):
        if self.current_loras == selected_loras and self.current_weights_map == weights_map:
            return

        print("ğŸ¸ æ­£åœ¨é…ç½® LoRA (ç‹¬ç«‹æƒé‡æ¨¡å¼)...")
        try:
            pipe.unload_lora_weights()
        except Exception:
            pass

        if not selected_loras:
            self.current_loras = []
            self.current_weights_map = {}
            return

        active_adapters = []
        adapter_weights = []

        for lora_file in selected_loras:
            adapter_name = re.sub(r"[^a-zA-Z0-9_]", "_", os.path.splitext(lora_file)[0])
            weight = weights_map.get(lora_file, 1.0)
            
            try:
                pipe.load_lora_weights(LORA_ROOT, weight_name=lora_file, adapter_name=adapter_name)
                active_adapters.append(adapter_name)
                adapter_weights.append(weight)
            except Exception as e:
                print(f"âš ï¸ LoRA {lora_file} åŠ è½½å¤±è´¥: {e}")

        if active_adapters:
            pipe.set_adapters(active_adapters, adapter_weights=adapter_weights)
        
        self.current_loras = list(selected_loras)
        self.current_weights_map = dict(weights_map)

    def get_pipeline(self, t_choice, v_choice, selected_loras, weights_map, mode='txt'):
        need_rebuild = (
            self.pipe is None or
            self.current_state["mode"] != mode or
            self.current_state["t_choice"] != t_choice or
            self.current_state["v_choice"] != v_choice
        )

        if need_rebuild:
            self._clear_pipeline() 
            try:
                temp_pipe = self._init_pipeline_base(mode)
                temp_pipe = self._inject_components(temp_pipe, t_choice, v_choice)
                
                if DEVICE == "cuda":
                    print("âš¡ å¯ç”¨ GPU æ˜¾å­˜åˆ†ç‰‡åŠ è½½")
                    temp_pipe.enable_sequential_cpu_offload()
                
                self.pipe = temp_pipe
                
                self.current_state = {
                    "mode": mode,
                    "t_choice": t_choice,
                    "v_choice": v_choice
                }
                self.current_loras = [] 
                self.current_weights_map = {}
                
            except Exception as e:
                self._clear_pipeline()
                raise gr.Error(f"æ¨¡å‹åŠ è½½å´©æºƒ: {str(e)}\nè¯·æ£€æŸ¥æ˜¾å­˜æˆ–æ¨¡å‹æ–‡ä»¶ã€‚")

        self._apply_loras(self.pipe, selected_loras, weights_map)
        return self.pipe

manager = ModelManager()

# ==========================================
# è¿›åº¦å›è°ƒ
# ==========================================
def make_progress_callback(progress, total_steps, refresh_interval=2):
    def _callback(pipe, step, timestep, callback_kwargs):
        global is_interrupted
        if is_interrupted:
            raise gr.Error("ğŸ›‘ ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢")
        step_idx = step + 1
        frac = step_idx / total_steps
        status_suffix = ""
        if step_idx % refresh_interval == 0 or step_idx == total_steps:
            _, mem_status = get_vram_info()
            status_suffix = f"\n{mem_status}"
        progress(frac, desc=f"Diffusion Step {step_idx}/{total_steps}{status_suffix}")
        return callback_kwargs
    return _callback

# ==========================================
# æ ¸å¿ƒé€»è¾‘ (è§£æç‹¬ç«‹æ§ä»¶ä¼ å…¥çš„å‚æ•°)
# ==========================================
def process_lora_inputs(lora_checks, lora_weights):
    selected = []
    weights_map = {}
    for i, fname in enumerate(LORA_FILES):
        if i < len(lora_checks) and lora_checks[i]:
            selected.append(fname)
            if i < len(lora_weights):
                weights_map[fname] = lora_weights[i]
            else:
                weights_map[fname] = 1.0
    return selected, weights_map

# ã€æ–°å¢ã€‘æ›´æ–° Prompt UI çš„è¾…åŠ©å‡½æ•°
def update_prompt_ui_base(prompt, *lora_ui_args):
    """
    lora_ui_args åŒ…å«: checks (Nä¸ª) + weights (Nä¸ª)
    """
    num_loras = len(LORA_FILES)
    if num_loras == 0:
        return prompt

    checks = lora_ui_args[:num_loras]
    weights = lora_ui_args[num_loras:num_loras*2]

    # æ¸…é™¤æ—§çš„ lora æ ‡ç­¾
    clean_p = re.sub(r"\s*<lora:[^>]+>", "", prompt or "").strip()
    
    new_tags = []
    for i, fname in enumerate(LORA_FILES):
        if i < len(checks) and checks[i]:
            w = weights[i] if i < len(weights) else 1.0
            name = os.path.splitext(fname)[0]
            alpha_str = f"{w:.2f}".rstrip("0").rstrip(".")
            new_tags.append(f"<lora:{name}:{alpha_str}>")
    
    if new_tags:
        return f"{clean_p} {' '.join(new_tags)}"
    else:
        return clean_p

# ã€ä¿®å¤ã€‘ä½¿ç”¨ *args æ¥æ”¶å‚æ•°ï¼Œé¿å… Gradio ä¼ å‚é¡ºåºé—®é¢˜
def run_inference(*args):
    global is_interrupted
    is_interrupted = False
    
    # è§£æå‚æ•°é¡ºåº
    # [prompt, checks(N), weights(N), t, v, w, h, steps, cfg, seed, random, batch, vram_th]
    idx = 0
    prompt = args[idx]; idx += 1
    num_loras = len(LORA_FILES)
    lora_checks = args[idx : idx+num_loras]; idx += num_loras
    lora_weights = args[idx : idx+num_loras]; idx += num_loras
    
    t_choice = args[idx]; idx += 1
    v_choice = args[idx]; idx += 1
    w = args[idx]; idx += 1
    h = args[idx]; idx += 1
    steps = args[idx]; idx += 1
    cfg = args[idx]; idx += 1
    seed = args[idx]; idx += 1
    is_random = args[idx]; idx += 1
    batch_size = args[idx]; idx += 1
    vram_threshold = args[idx]; idx += 1

    auto_flush_vram(vram_threshold)
    clean_w = (int(w) // 16) * 16
    clean_h = (int(h) // 16) * 16
    
    selected_loras, weights_map = process_lora_inputs(lora_checks, lora_weights)
    
    # æ„å»ºæœ€ç»ˆ Prompt
    if selected_loras:
        tags = []
        for f in selected_loras:
            w_val = weights_map.get(f, 1.0)
            name = os.path.splitext(f)[0]
            tags.append(f"<lora:{name}:{w_val:.2f}>")
        clean_p = re.sub(r"\s*<lora:[^>]+>", "", prompt or "").strip()
        final_prompt = f"{clean_p} {' '.join(tags)}"
    else:
        final_prompt = prompt

    try:
        pipe = manager.get_pipeline(t_choice, v_choice, selected_loras, weights_map, mode='txt')
    except Exception as e:
        raise gr.Error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

    if is_random: seed = random.randint(0, 2**32 - 1)
    generator = torch.Generator(DEVICE).manual_seed(int(seed))

    date_folder = datetime.now().strftime("%Y-%m-%d")
    save_dir = os.path.join(OUTPUT_ROOT, date_folder)
    os.makedirs(save_dir, exist_ok=True)

    results_images = []
    progress = gr.Progress()

    try:
        print(f"ğŸ”¥ ä»»åŠ¡å¯åŠ¨ | å›¾ç‰‡åˆ†è¾¨ç‡: {clean_w}x{clean_h} | ç§å­: {seed}")
        step_callback = make_progress_callback(progress, int(steps))

        for i in range(int(batch_size)):
            if is_interrupted: break
            output = pipe(
                prompt=final_prompt,
                width=clean_w,
                height=clean_h,
                num_inference_steps=int(steps),
                guidance_scale=float(cfg),
                generator=generator,
                callback_on_step_end=step_callback
            ).images[0]

            filename = f"{datetime.now().strftime('%H%M%S')}_{uuid.uuid4().hex[:4]}.png"
            path = os.path.join(save_dir, filename)
            output.save(path)
            results_images.append(output)
            _, current_status = get_vram_info()
            yield results_images, seed, current_status

    except Exception as e:
        if "ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢" in str(e):
            print("ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
        else:
            import traceback
            traceback.print_exc()
            raise gr.Error(f"ç”Ÿæˆä¸­æ–­: {str(e)}")
    finally:
        # del pipe
        auto_flush_vram(vram_threshold)

# ã€ä¿®å¤ã€‘å›¾ç”Ÿå›¾
def run_img2img(*args, progress=gr.Progress()):
    global is_interrupted
    is_interrupted = False
    
    # [input_image, prompt, checks(N), weights(N), ...fixed...]
    idx = 0
    input_image = args[idx]; idx += 1
    prompt = args[idx]; idx += 1
    
    num_loras = len(LORA_FILES)
    lora_checks = args[idx : idx+num_loras]; idx += num_loras
    lora_weights = args[idx : idx+num_loras]; idx += num_loras
    
    t_choice = args[idx]; idx += 1
    v_choice = args[idx]; idx += 1
    output_width = args[idx]; idx += 1
    output_height = args[idx]; idx += 1
    strength = args[idx]; idx += 1
    steps = args[idx]; idx += 1
    cfg = args[idx]; idx += 1
    seed = args[idx]; idx += 1
    is_random = args[idx]; idx += 1
    batch_size = args[idx]; idx += 1
    vram_threshold = args[idx]; idx += 1

    if input_image is None:
        raise gr.Error("âŒ è¯·å…ˆä¸Šä¼ å›¾ç‰‡")
        
    auto_flush_vram(vram_threshold)
    selected_loras, weights_map = process_lora_inputs(lora_checks, lora_weights)

    if selected_loras:
        tags = []
        for f in selected_loras:
            w_val = weights_map.get(f, 1.0)
            name = os.path.splitext(f)[0]
            tags.append(f"<lora:{name}:{w_val:.2f}>")
        clean_p = re.sub(r"\s*<lora:[^>]+>", "", prompt or "").strip()
        final_prompt = f"{clean_p} {' '.join(tags)}"
    else:
        final_prompt = prompt

    if output_width == 0 or output_height == 0:
        orig_w, orig_h = input_image.size
        aspect = orig_w / orig_h
        target_size = 1024
        if aspect > 1:
            target_w, target_h = target_size, max(512, int(target_size / aspect))
        else:
            target_h, target_w = target_size, max(512, int(target_size * aspect))
        target_w = (target_w // 16) * 16
        target_h = (target_h // 16) * 16
    else:
        target_w = (int(output_width) // 16) * 16
        target_h = (int(output_height) // 16) * 16

    input_image = input_image.convert("RGB").resize((target_w, target_h))

    if is_random: seed = random.randint(0, 2**32 - 1)
    generator = torch.Generator(DEVICE).manual_seed(int(seed))

    date_folder = datetime.now().strftime("%Y-%m-%d")
    save_dir = os.path.join(OUTPUT_ROOT, date_folder)
    os.makedirs(save_dir, exist_ok=True)

    results = []
    pipe = None
    
    try:
        pipe = manager.get_pipeline(t_choice, v_choice, selected_loras, weights_map, mode='img')

        for i in progress.tqdm(range(int(batch_size)), desc="å›¾ç”Ÿå›¾ç”Ÿæˆä¸­"):
            if is_interrupted: break
            torch.cuda.ipc_collect()
            step_callback = make_progress_callback(progress, int(steps))

            output = pipe(
                prompt=final_prompt,
                image=input_image,
                strength=float(strength),
                num_inference_steps=int(steps),
                guidance_scale=0.0,
                generator=generator,
                callback_on_step_end=step_callback
            ).images[0]

            filename = f"img2img_{datetime.now().strftime('%H%M%S')}_{uuid.uuid4().hex[:4]}.png"
            path = os.path.join(save_dir, filename)
            output.save(path)
            results.append(path)

    except Exception as e:
        if "ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢" in str(e):
            print("ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
        else:
            import traceback
            traceback.print_exc()
            raise gr.Error(f"ç”Ÿæˆä¸­æ–­: {str(e)}")
    finally:
        del pipe
        auto_flush_vram(vram_threshold)
        _, current_status = get_vram_info()

    return results, seed, current_status

# ã€ä¿®å¤ã€‘èåˆå›¾
def run_fusion_img(*args, progress=gr.Progress()):
    global is_interrupted
    is_interrupted = False
    
    # [image1, image2, prompt, checks(N), weights(N), ...fixed...]
    idx = 0
    image1 = args[idx]; idx += 1
    image2 = args[idx]; idx += 1
    prompt = args[idx]; idx += 1
    
    num_loras = len(LORA_FILES)
    lora_checks = args[idx : idx+num_loras]; idx += num_loras
    lora_weights = args[idx : idx+num_loras]; idx += num_loras
    
    t_choice = args[idx]; idx += 1
    v_choice = args[idx]; idx += 1
    output_width = args[idx]; idx += 1
    output_height = args[idx]; idx += 1
    blend_strength = args[idx]; idx += 1
    strength = args[idx]; idx += 1
    steps = args[idx]; idx += 1
    cfg = args[idx]; idx += 1
    seed = args[idx]; idx += 1
    is_random = args[idx]; idx += 1
    batch_size = args[idx]; idx += 1
    vram_threshold = args[idx]; idx += 1

    if image1 is None or image2 is None:
        raise gr.Error("âŒ è¯·ä¸Šä¼ ä¸¤å¼ å‚è€ƒå›¾ç‰‡")
        
    auto_flush_vram(vram_threshold)
    selected_loras, weights_map = process_lora_inputs(lora_checks, lora_weights)

    if selected_loras:
        tags = []
        for f in selected_loras:
            w_val = weights_map.get(f, 1.0)
            name = os.path.splitext(f)[0]
            tags.append(f"<lora:{name}:{w_val:.2f}>")
        clean_p = re.sub(r"\s*<lora:[^>]+>", "", prompt or "").strip()
        final_prompt = f"{clean_p} {' '.join(tags)}"
    else:
        final_prompt = prompt

    if output_width == 0 or output_height == 0:
        orig_w, orig_h = image1.size
        aspect = orig_w / orig_h
        target_size = 1024
        if aspect > 1:
            target_w, target_h = target_size, max(512, int(target_size / aspect))
        else:
            target_h, target_w = target_size, max(512, int(target_size * aspect))
        target_w = (target_w // 16) * 16
        target_h = (target_h // 16) * 16
    else:
        target_w = (int(output_width) // 16) * 16
        target_h = (int(output_height) // 16) * 16

    image1 = image1.convert("RGB").resize((target_w, target_h))
    image2 = image2.convert("RGB").resize((target_w, target_h))
    blended_image = Image.blend(image1, image2, float(blend_strength))

    if is_random: seed = random.randint(0, 2**32 - 1)
    generator = torch.Generator(DEVICE).manual_seed(int(seed))

    date_folder = datetime.now().strftime("%Y-%m-%d")
    save_dir = os.path.join(OUTPUT_ROOT, date_folder)
    os.makedirs(save_dir, exist_ok=True)

    results = []
    pipe = None
    
    try:
        pipe = manager.get_pipeline(t_choice, v_choice, selected_loras, weights_map, mode='img')

        for i in progress.tqdm(range(int(batch_size)), desc="èåˆç”Ÿæˆä¸­"):
            if is_interrupted: break
            torch.cuda.ipc_collect()
            step_callback = make_progress_callback(progress, int(steps))

            output = pipe(
                prompt=final_prompt,
                image=blended_image,
                strength=float(strength),
                num_inference_steps=int(steps),
                guidance_scale=0.0,
                generator=generator,
                callback_on_step_end=step_callback
            ).images[0]

            filename = f"fusion_{datetime.now().strftime('%H%M%S')}_{uuid.uuid4().hex[:4]}.png"
            path = os.path.join(save_dir, filename)
            output.save(path)
            results.append(path)

    except Exception as e:
        if "ä»»åŠ¡å·²æ‰‹åŠ¨åœæ­¢" in str(e):
            print("ğŸ›‘ ä»»åŠ¡å·²åœæ­¢")
        else:
            import traceback
            traceback.print_exc()
            raise gr.Error(f"ç”Ÿæˆä¸­æ–­: {str(e)}")
    finally:
        del pipe
        auto_flush_vram(vram_threshold)
        _, current_status = get_vram_info()

    return results, seed, current_status

# ==========================================
# UI ç•Œé¢
# ==========================================
# ã€æ–°å¢ã€‘å®šä¹‰JSé€€å‡ºè„šæœ¬ï¼šå…³é—­çª—å£æˆ–æ˜¾ç¤ºé»‘å±
js_kill_window = """
function() {
    // å°è¯•å…³é—­çª—å£
    setTimeout(function(){ window.close(); }, 1000);
    // å¦‚æœæ— æ³•å…³é—­ï¼Œåˆ™è¦†ç›–é¡µé¢æ˜¾ç¤ºæç¤º
    document.body.innerHTML = '<div style="display:flex;justify-content:center;align-items:center;height:100vh;background:#000;color:#fff;font-family:sans-serif;"><h1>ğŸš« ç³»ç»Ÿå·²å…³é—­ï¼Œè¯·ç›´æ¥å…³é—­æ­¤æ ‡ç­¾é¡µ</h1></div>';
    document.body.style.backgroundColor = "black";
    document.body.style.overflow = "hidden";
    return [];
}
"""

# JSé€€å‡ºè„šæœ¬ï¼šå…³é—­çª—å£æˆ–æ˜¾ç¤ºé»‘å±
js_kill_window = """
function() {
    // å°è¯•å…³é—­çª—å£
    setTimeout(function(){ window.close(); }, 1000);
    // å¦‚æœæ— æ³•å…³é—­ï¼Œåˆ™è¦†ç›–é¡µé¢æ˜¾ç¤ºæç¤º
    document.body.innerHTML = '<div style="display:flex;justify-content:center;align-items:center;height:100vh;background:#000;color:#fff;font-family:sans-serif;"><h1>ğŸš« ç³»ç»Ÿå·²å…³é—­ï¼Œè¯·ç›´æ¥å…³é—­æ­¤æ ‡ç­¾é¡µ</h1></div>';
    document.body.style.backgroundColor = "black";
    document.body.style.overflow = "hidden";
    return [];
}
"""

# Pythoné€€å‡ºå‡½æ•°ï¼šå…³é—­è¿›ç¨‹
def kill_system_process():
    print("ğŸ›‘ æ­£åœ¨æ‰§è¡Œä¸€é”®é€€å‡ºç¨‹åº...")
    try:
        # 1. ä¼˜å…ˆå…³é—­å¯åŠ¨å™¨ (Windows)
        os.system("taskkill /F /IM Z-Image-Launcher.exe")
    except Exception:
        pass

    # å»¶è¿Ÿ1ç§’ï¼Œç¡®ä¿å‰ç«¯JSæœ‰æœºä¼šæ‰§è¡Œ
    time.sleep(1)

    try:
        # 2. å¼ºåˆ¶æ€æ‰æ‰€æœ‰ Python è¿›ç¨‹
        os.system("taskkill /F /IM python.exe")
    except Exception:
        pass

    # 3. æœ€åè‡ªæ€ï¼ˆå¦‚æœä¸Šé¢æ²¡æ€æ‰è‡ªå·±çš„è¯ï¼‰
    sys.exit(0)

with gr.Blocks(title="é€ ç›¸ Z-Image Pro Studio | ä½œè€…: ") as demo:

    print('\n' + '!'*60)
    print('  æœ¬è½¯ä»¶ç”± Leewheel å…è´¹åˆ†äº«ï¼Œä¸¥ç¦å”®å–ï¼')
    print('!'*60 + '\n')
    gr.Warning('æœ¬è½¯ä»¶ç”± Leewheel å…è´¹åˆ†äº«ã€‚å¦‚æœä½ æ˜¯ä»˜è´¹è´­ä¹°ï¼Œä½ è¢«éª—äº†ï¼', duration=20)
    # ã€ä¿®æ”¹ã€‘é¡¶éƒ¨å¢åŠ ä¸€é”®é€€å‡ºæŒ‰é’®
    with gr.Row(elem_id="header_row"):
        gr.Markdown("# ğŸ¨ é€ ç›¸ Z-Image Pro Studio | ä½œè€…:  Leewheel(V1.00C)")
        exit_btn = gr.Button("âŒ ä¸€é”®é€€å‡ºç³»ç»Ÿ", variant="stop", scale=0, min_width=150)
        
    vram_info_display = gr.Markdown("æ˜¾å­˜çŠ¶æ€åŠ è½½ä¸­...")

    with gr.Tabs():
        # --- æ–‡æˆå›¾ ---
        with gr.Tab("æ–‡æˆå›¾"):
            with gr.Row():
                with gr.Column(scale=4):
                    prompt_input = gr.Textbox(label="Prompt", lines=4)
                    manual_flush_btn = gr.Button("ğŸ§¹ æ¸…ç†æ˜¾å­˜", size="sm", variant="secondary")
                    vram_threshold_slider = gr.Slider(50, 98, 90, step=1, label="è‡ªåŠ¨æ¸…ç†é˜ˆå€¼ (%)")
                    
                    # ã€æ ¸å¿ƒä¿®æ”¹ã€‘åŠ¨æ€ç”Ÿæˆæ¯ä¸ª LoRA çš„æ§ä»¶
                    with gr.Accordion("LoRA æƒé‡è®¾ç½® (æ¯ä¸ª LoRA ç‹¬ç«‹è°ƒèŠ‚)", open=False):
                        txt_lora_checks = []
                        txt_lora_sliders = []
                        
                        if not LORA_FILES:
                            gr.Markdown("*æœªæ£€æµ‹åˆ° LoRA æ–‡ä»¶*")
                        else:
                            for fname in LORA_FILES:
                                with gr.Row():
                                    # å¤é€‰æ¡†
                                    chk = gr.Checkbox(label=fname, value=False, scale=1, container=False)
                                    # æ»‘å—
                                    sld = gr.Slider(0, 2.0, 1.0, step=0.05, label="æƒé‡", scale=4)
                                    txt_lora_checks.append(chk)
                                    txt_lora_sliders.append(sld)

                    with gr.Accordion("æ¨¡å‹è®¾ç½®", open=True):
                        refresh_models_btn = gr.Button("ğŸ”„ åˆ·æ–°åº•æ¨¡/VAE", size="sm")
                        t_drop = gr.Dropdown(label="Transformer", choices=["default"] + scan_model_items(MOD_TRANS_DIR), value="default")
                        v_drop = gr.Dropdown(label="VAE", choices=["default"] + scan_model_items(MOD_VAE_DIR), value="default")
                        with gr.Row():
                            width_s = gr.Slider(512, 2048, 1024, step=16, label="å®½ (16å€æ•°)")
                            height_s = gr.Slider(512, 2048, 1024, step=16, label="é«˜ (16å€æ•°)")
                        step_s = gr.Slider(1, 50, 8, label="æ­¥æ•°")
                        cfg_s = gr.Slider(0, 10, 0, label="CFG")
                        batch_s = gr.Slider(1, 32, 1, step=1, label="ç”Ÿæˆå¼ æ•°")
                        seed_n = gr.Number(label="ç§å­", value=42, precision=0)
                        random_c = gr.Checkbox(label="éšæœºç§å­", value=True)

                    with gr.Row():
                        run_btn = gr.Button("ğŸš€ å¼€å§‹ç”Ÿæˆ", variant="primary", size="lg")
                        stop_btn = gr.Button("ğŸ›‘ åœæ­¢ç”Ÿæˆ", variant="stop", size="lg", interactive=False)

                with gr.Column(scale=6):
                    res_gallery = gr.Gallery(label="è¾“å‡ºç»“æœ", columns=2, height="80vh")
                    res_seed = gr.Number(label="ç§å­", interactive=False)
                    vram_info_display = gr.Markdown("æ˜¾å­˜çŠ¶æ€åŠ è½½ä¸­...")

        # --- å›¾ç‰‡ç¼–è¾‘ ---
        with gr.Tab("å›¾ç‰‡ç¼–è¾‘"):
            with gr.Row():
                with gr.Column():
                    image_input = gr.Image(label="ä¸Šä¼ å›¾ç‰‡", type="pil")
                    with gr.Group():
                        rotate_angle = gr.Slider(-360, 360, 0, step=1, label="æ—‹è½¬è§’åº¦ (åº¦)")
                        crop_x = gr.Slider(0, 100, 0, step=1, label="è£å‰ª X (%)")
                        crop_y = gr.Slider(0, 100, 0, step=1, label="è£å‰ª Y (%)")
                        crop_width = gr.Slider(0, 100, 100, step=1, label="è£å‰ªå®½åº¦ (%)")
                        crop_height = gr.Slider(0, 100, 100, step=1, label="è£å‰ªé«˜åº¦ (%)")
                        flip_horizontal = gr.Checkbox(label="æ°´å¹³ç¿»è½¬")
                        flip_vertical = gr.Checkbox(label="å‚ç›´ç¿»è½¬")
                    edit_btn = gr.Button("å¼€å§‹ç¼–è¾‘", variant="primary")
                with gr.Column():
                    edited_image_output = gr.Image(label="ç¼–è¾‘åçš„å›¾ç‰‡", type="pil")
                    with gr.Group():
                        apply_filter = gr.Dropdown(["æ¨¡ç³Š", "è½®å»“", "ç»†èŠ‚", "è¾¹ç¼˜å¢å¼º", "æ›´å¤šè¾¹ç¼˜å¢å¼º", "æµ®é›•", "æŸ¥æ‰¾è¾¹ç¼˜", "é”åŒ–", "å¹³æ»‘", "æ›´å¤šå¹³æ»‘"], label="åº”ç”¨æ»¤é•œ")
                        brightness = gr.Slider(-100, 100, 0, step=1, label="äº®åº¦è°ƒæ•´ (%)")
                        contrast = gr.Slider(-100, 100, 0, step=1, label="å¯¹æ¯”åº¦è°ƒæ•´ (%)")
                        saturation = gr.Slider(-100, 100, 0, step=1, label="é¥±å’Œåº¦è°ƒæ•´ (%)")

            def edit_image(image, angle, x, y, width, height, hflip, vflip, filter, brightness, contrast, saturation):
                if image is None: return None
                if angle != 0: image = image.rotate(angle, expand=True)
                if x or y or width < 100 or height < 100:
                    original_width, original_height = image.size
                    left = int(original_width * x / 100)
                    top = int(original_height * y / 100)
                    right = int(original_width * (x + width) / 100)
                    bottom = int(original_height * (y + height) / 100)
                    image = image.crop((left, top, right, bottom))
                if hflip: image = ImageOps.mirror(image)
                if vflip: image = ImageOps.flip(image)
                if filter:
                    filter_map = {
                        "æ¨¡ç³Š": ImageFilter.BLUR, "è½®å»“": ImageFilter.CONTOUR, "ç»†èŠ‚": ImageFilter.DETAIL,
                        "è¾¹ç¼˜å¢å¼º": ImageFilter.EDGE_ENHANCE, "æ›´å¤šè¾¹ç¼˜å¢å¼º": ImageFilter.EDGE_ENHANCE_MORE,
                        "æµ®é›•": ImageFilter.EMBOSS, "æŸ¥æ‰¾è¾¹ç¼˜": ImageFilter.FIND_EDGES,
                        "é”åŒ–": ImageFilter.SHARPEN, "å¹³æ»‘": ImageFilter.SMOOTH, "æ›´å¤šå¹³æ»‘": ImageFilter.SMOOTH_MORE
                    }
                    filter_func = filter_map.get(filter)
                    if filter_func: image = image.filter(filter_func)
                if brightness != 0:
                    enhancer = ImageEnhance.Brightness(image)
                    image = enhancer.enhance(1 + brightness / 100)
                if contrast != 0:
                    enhancer = ImageEnhance.Contrast(image)
                    image = enhancer.enhance(1 + contrast / 100)
                if saturation != 0:
                    enhancer = ImageEnhance.Color(image)
                    image = enhancer.enhance(1 + saturation / 100)
                return image

            edit_btn.click(
                fn=edit_image,
                inputs=[image_input, rotate_angle, crop_x, crop_y, crop_width, crop_height, flip_horizontal, flip_vertical, apply_filter, brightness, contrast, saturation],
                outputs=edited_image_output
            )

        # --- å›¾ç”Ÿå›¾ ---
        with gr.Tab("å›¾ç”Ÿå›¾"):
            with gr.Row():
                with gr.Column(scale=4):
                    with gr.Group():
                        img2img_input = gr.Image(label="ä¸Šä¼ å‚è€ƒå›¾", type="pil")
                        img2img_prompt = gr.Textbox(label="Prompt (æ¨è)", lines=2, placeholder="æè¿°ä½ æƒ³è¦ç”Ÿæˆçš„ç”»é¢...")
                        img2img_flush = gr.Button("ğŸ§¹ æ¸…ç†æ˜¾å­˜", size="sm", variant="secondary")
                        
                        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘å›¾ç”Ÿå›¾ç‹¬ç«‹æ§ä»¶
                        with gr.Accordion("LoRA æƒé‡è®¾ç½® (ç‹¬ç«‹è°ƒèŠ‚)", open=False):
                            i2i_lora_checks = []
                            i2i_lora_sliders = []
                            if not LORA_FILES:
                                gr.Markdown("*æœªæ£€æµ‹åˆ° LoRA æ–‡ä»¶*")
                            else:
                                for fname in LORA_FILES:
                                    with gr.Row():
                                        chk = gr.Checkbox(label=fname, value=False, scale=1, container=False)
                                        sld = gr.Slider(0, 2.0, 1.0, step=0.05, label="æƒé‡", scale=4)
                                        i2i_lora_checks.append(chk)
                                        i2i_lora_sliders.append(sld)

                    with gr.Accordion("æ¨¡å‹ä¸å‚æ•°", open=True):
                        img2img_refresh_models = gr.Button("ğŸ”„ åˆ·æ–°åº•æ¨¡/VAE", size="sm")
                        img2img_t_drop = gr.Dropdown(label="Transformer", choices=["default"] + scan_model_items(MOD_TRANS_DIR), value="default")
                        img2img_v_drop = gr.Dropdown(label="VAE", choices=["default"] + scan_model_items(MOD_VAE_DIR), value="default")
                        with gr.Row():
                            img2img_width_s = gr.Slider(0, 2048, 0, step=16, label="è¾“å‡ºå®½ (0=è‡ªåŠ¨ä¿æŒæ¯”ä¾‹)")
                            img2img_height_s = gr.Slider(0, 2048, 0, step=16, label="è¾“å‡ºé«˜ (0=è‡ªåŠ¨ä¿æŒæ¯”ä¾‹)")
                        gr.Markdown("**æç¤ºï¼š** å®½é«˜éƒ½ä¸º0æ—¶è‡ªåŠ¨ä¿æŒä¸Šä¼ å›¾æ¯”ä¾‹å¹¶æ¥è¿‘1024ï¼›æ‰‹åŠ¨è®¾ç½®å¤§äº512æ—¶ç”Ÿæ•ˆ")
                        img2img_strength = gr.Slider(0.0, 1.0, 0.75, step=0.01, label="é‡ç»˜å¼ºåº¦")
                        img2img_steps = gr.Slider(1, 100, 12, step=1, label="æ­¥æ•°")
                        img2img_cfg = gr.Number(value=0.0, label="CFGï¼ˆTurboæ¨¡å‹å›ºå®šä¸º0.0ï¼‰", interactive=False)
                        img2img_batch = gr.Slider(1, 8, 1, step=1, label="å¼ æ•°")
                        img2img_seed = gr.Number(label="ç§å­", value=42, precision=0)
                        img2img_random = gr.Checkbox(label="éšæœºç§å­", value=True)
                    with gr.Row():
                        img2img_run_btn = gr.Button("ğŸš€ ç”Ÿæˆ", variant="primary", size="lg")
                        img2img_stop_btn = gr.Button("ğŸ›‘ åœæ­¢", variant="stop", size="lg", interactive=False)
                with gr.Column(scale=6):
                    img2img_gallery = gr.Gallery(label="å›¾ç”Ÿå›¾ç»“æœ", columns=2, height="80vh")
                    img2img_res_seed = gr.Number(label="ç§å­", interactive=False)

        # --- èåˆå›¾ ---
        with gr.Tab("èåˆå›¾"):
            gr.Markdown("**èåˆ2å¼ å›¾ç‰‡**ï¼šå›¾ç‰‡1æä¾›ä¸»è¦ç»“æ„/å§¿åŠ¿ï¼Œå›¾ç‰‡2æä¾›ç»†èŠ‚/è„¸éƒ¨/é£æ ¼ã€‚")
            with gr.Row():
                with gr.Column(scale=4):
                    with gr.Group():
                        fusion_input1 = gr.Image(label="å›¾ç‰‡1ï¼ˆä¸»ç»“æ„/å§¿åŠ¿ï¼‰", type="pil")
                        fusion_input2 = gr.Image(label="å›¾ç‰‡2ï¼ˆç»†èŠ‚/è„¸éƒ¨/é£æ ¼ï¼‰", type="pil")
                        fusion_prompt = gr.Textbox(label="èåˆæè¿° Prompt", lines=3)
                        fusion_flush = gr.Button("ğŸ§¹ æ¸…ç†æ˜¾å­˜", size="sm", variant="secondary")
                        
                        # ã€æ ¸å¿ƒä¿®æ”¹ã€‘èåˆå›¾ç‹¬ç«‹æ§ä»¶
                        with gr.Accordion("LoRA æƒé‡è®¾ç½® (ç‹¬ç«‹è°ƒèŠ‚)", open=False):
                            fusion_lora_checks = []
                            fusion_lora_sliders = []
                            if not LORA_FILES:
                                gr.Markdown("*æœªæ£€æµ‹åˆ° LoRA æ–‡ä»¶*")
                            else:
                                for fname in LORA_FILES:
                                    with gr.Row():
                                        chk = gr.Checkbox(label=fname, value=False, scale=1, container=False)
                                        sld = gr.Slider(0, 2.0, 1.0, step=0.05, label="æƒé‡", scale=4)
                                        fusion_lora_checks.append(chk)
                                        fusion_lora_sliders.append(sld)

                    with gr.Accordion("æ¨¡å‹ä¸å‚æ•°", open=True):
                        fusion_refresh_models = gr.Button("ğŸ”„ åˆ·æ–°åº•æ¨¡/VAE", size="sm")
                        fusion_t_drop = gr.Dropdown(label="Transformer", choices=["default"] + scan_model_items(MOD_TRANS_DIR), value="default")
                        fusion_v_drop = gr.Dropdown(label="VAE", choices=["default"] + scan_model_items(MOD_VAE_DIR), value="default")
                        with gr.Row():
                            fusion_width_s = gr.Slider(0, 2048, 0, step=16, label="è¾“å‡ºå®½ (0=è‡ªåŠ¨ä¿æŒæ¯”ä¾‹)")
                            fusion_height_s = gr.Slider(0, 2048, 0, step=16, label="è¾“å‡ºé«˜ (0=è‡ªåŠ¨ä¿æŒæ¯”ä¾‹)")
                        gr.Markdown("**æç¤ºï¼š** å®½é«˜éƒ½ä¸º0æ—¶è‡ªåŠ¨ä¿æŒå›¾ç‰‡1æ¯”ä¾‹å¹¶æ¥è¿‘1024")
                        with gr.Row():
                            fusion_blend = gr.Slider(0.0, 1.0, 0.5, step=0.05, label="å›¾ç‰‡2æ··åˆå¼ºåº¦ (0=å…¨ç”¨å›¾ç‰‡1, 1=å…¨ç”¨å›¾ç‰‡2)")
                            fusion_strength = gr.Slider(0.0, 1.0, 0.7, step=0.05, label="é‡ç»˜å¼ºåº¦ (è¶Šé«˜å˜åŒ–è¶Šå¤§)")
                        fusion_steps = gr.Slider(1, 100, 15, step=1, label="æ­¥æ•°")
                        fusion_cfg = gr.Number(value=0.0, label="CFGï¼ˆå›ºå®šä¸º0.0ï¼‰", interactive=False)
                        fusion_batch = gr.Slider(1, 8, 1, step=1, label="å¼ æ•°")
                        fusion_seed = gr.Number(label="ç§å­", value=42, precision=0)
                        fusion_random = gr.Checkbox(label="éšæœºç§å­", value=True)
                    with gr.Row():
                        fusion_run_btn = gr.Button("ğŸš€ å¼€å§‹èåˆ", variant="primary", size="lg")
                        fusion_stop_btn = gr.Button("ğŸ›‘ åœæ­¢", variant="stop", size="lg", interactive=False)
                with gr.Column(scale=6):
                    fusion_gallery = gr.Gallery(label="èåˆç»“æœ", columns=2, height="80vh")
                    fusion_res_seed = gr.Number(label="ç§å­", interactive=False)

    # -----------------------
    # UIçŠ¶æ€å‡½æ•°
    # -----------------------
    def ui_to_running():
        return gr.update(interactive=False), gr.update(interactive=True)

    def ui_to_idle():
        return gr.update(interactive=True), gr.update(interactive=False)

    def trigger_interrupt():
        global is_interrupted
        is_interrupted = True
        return "ğŸ›‘ æ­£åœ¨å¼ºåˆ¶ä¸­æ–­..."

    # -----------------------
    # æŒ‰é’®äº‹ä»¶ç»‘å®š
    # -----------------------
    
    # ã€æ–°å¢ã€‘é€€å‡ºæŒ‰é’®ç»‘å®šäº‹ä»¶
    # 1. å…ˆè§¦å‘ _js æ‰§è¡Œå‰ç«¯æ¸…ç†ï¼ˆå˜é»‘ã€å°è¯•å…³é—­çª—å£ï¼‰
    # 2. ç„¶åè§¦å‘ fn æ‰§è¡Œåç«¯æ€è¿›ç¨‹
    # åˆå¹¶é€»è¾‘ï¼šä¸€æ¬¡ç‚¹å‡»åŒæ—¶è§¦å‘ JS å’Œ Python
    exit_btn.click(
        fn=kill_system_process,   # åç«¯ï¼šæ€è¿›ç¨‹
        js=js_kill_window         # å‰ç«¯ï¼šå…³ç½‘é¡µæˆ–æ˜¾ç¤ºé»‘å±
    )

    # æ–‡ç”Ÿå›¾
    refresh_models_btn.click(
        fn=lambda: (
            gr.update(choices=["default"] + scan_model_items(MOD_TRANS_DIR)),
            gr.update(choices=["default"] + scan_model_items(MOD_VAE_DIR))
        ),
        outputs=[t_drop, v_drop]
    )
    manual_flush_btn.click(
        fn=lambda: (gc.collect(), torch.cuda.empty_cache(), get_vram_info()[1])[2],
        outputs=vram_info_display
    )

    # ã€æ–°å¢ã€‘ç»‘å®šæ–‡ç”Ÿå›¾ Prompt è‡ªåŠ¨æ›´æ–°
    txt_ui_inputs = [prompt_input] + txt_lora_checks + txt_lora_sliders
    for c in txt_lora_checks + txt_lora_sliders:
        c.change(fn=update_prompt_ui_base, inputs=txt_ui_inputs, outputs=prompt_input)

    inference_event = run_btn.click(
        fn=ui_to_running, 
        outputs=[run_btn, stop_btn]
    ).then(
        fn=run_inference,
        inputs=txt_ui_inputs + [t_drop, v_drop, width_s, height_s, step_s, cfg_s, seed_n, random_c, batch_s, vram_threshold_slider],
        outputs=[res_gallery, res_seed, vram_info_display]
    ).then(
        fn=ui_to_idle,
        outputs=[run_btn, stop_btn]
    )

    stop_btn.click(
        fn=trigger_interrupt,
        outputs=vram_info_display
    ).then(
        fn=ui_to_idle,
        outputs=[run_btn, stop_btn],
        cancels=[inference_event]
    )
    
    # å›¾ç”Ÿå›¾
    def refresh_all_models_img():
        return gr.update(choices=["default"] + scan_model_items(MOD_TRANS_DIR)), gr.update(choices=["default"] + scan_model_items(MOD_VAE_DIR))
    img2img_refresh_models.click(fn=refresh_all_models_img, outputs=[img2img_t_drop, img2img_v_drop])
    img2img_flush.click(fn=lambda: (gc.collect(), torch.cuda.empty_cache(), get_vram_info()[1])[2], outputs=vram_info_display)

    # ã€æ–°å¢ã€‘ç»‘å®šå›¾ç”Ÿå›¾ Prompt è‡ªåŠ¨æ›´æ–°
    i2i_ui_inputs = [img2img_prompt] + i2i_lora_checks + i2i_lora_sliders
    for c in i2i_lora_checks + i2i_lora_sliders:
        c.change(fn=update_prompt_ui_base, inputs=i2i_ui_inputs, outputs=img2img_prompt)

    img2img_event = img2img_run_btn.click(fn=ui_to_running, outputs=[img2img_run_btn, img2img_stop_btn])\
        .then(fn=run_img2img,
              inputs=[img2img_input, img2img_prompt] + i2i_lora_checks + i2i_lora_sliders + 
                      [img2img_t_drop, img2img_v_drop, img2img_width_s, img2img_height_s,
                       img2img_strength, img2img_steps, img2img_cfg, img2img_seed, img2img_random, img2img_batch, vram_threshold_slider],
              outputs=[img2img_gallery, img2img_res_seed, vram_info_display])\
        .then(fn=ui_to_idle, outputs=[img2img_run_btn, img2img_stop_btn])

    img2img_stop_btn.click(fn=trigger_interrupt, outputs=vram_info_display).then(fn=ui_to_idle, outputs=[img2img_run_btn, img2img_stop_btn], cancels=[img2img_event])

    # èåˆå›¾
    fusion_refresh_models.click(fn=refresh_all_models_img, outputs=[fusion_t_drop, fusion_v_drop])
    fusion_flush.click(fn=lambda: (gc.collect(), torch.cuda.empty_cache(), get_vram_info()[1])[2], outputs=vram_info_display)

    # ã€æ–°å¢ã€‘ç»‘å®šèåˆå›¾ Prompt è‡ªåŠ¨æ›´æ–°
    fusion_ui_inputs = [fusion_prompt] + fusion_lora_checks + fusion_lora_sliders
    for c in fusion_lora_checks + fusion_lora_sliders:
        c.change(fn=update_prompt_ui_base, inputs=fusion_ui_inputs, outputs=fusion_prompt)

    fusion_event = fusion_run_btn.click(fn=ui_to_running, outputs=[fusion_run_btn, fusion_stop_btn])\
        .then(fn=run_fusion_img,
              inputs=[fusion_input1, fusion_input2, fusion_prompt] + fusion_lora_checks + fusion_lora_sliders + 
                      [fusion_t_drop, fusion_v_drop, fusion_width_s, fusion_height_s,
                       fusion_blend, fusion_strength, fusion_steps, fusion_cfg, 
                       fusion_seed, fusion_random, fusion_batch, vram_threshold_slider],
              outputs=[fusion_gallery, fusion_res_seed, vram_info_display])\
        .then(fn=ui_to_idle, outputs=[fusion_run_btn, fusion_stop_btn])

    fusion_stop_btn.click(fn=trigger_interrupt, outputs=vram_info_display).then(fn=ui_to_idle, outputs=[fusion_run_btn, fusion_stop_btn], cancels=[fusion_event])

if __name__ == "__main__":
    demo.launch(share=False, inbrowser=True)